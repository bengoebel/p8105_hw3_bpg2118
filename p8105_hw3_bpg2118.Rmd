---
title: "p8105_hw3_bpg2118"
author: "Benjamin Goebel"
date: "10/20/2021"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(here)
```

#### **Problem 1**

First, let's read in the instacart data.
```{r}
data("instacart")
```

This data set is `r format(nrow(instacart), big.mark = ",", trim = TRUE)` rows 
and `r ncol(instacart)` columns. 
Each observation in this data set represents a product in each order.
Some key variables include order_id, product_id, department_id and aisle_id; 
these are the respective IDs for the order, product, department and aisle.
There are 
`r format(length(unique(pull(instacart, order_id))), big.mark = ",", 
          trim = TRUE)` orders, 
`r format(length(unique(pull(instacart, product_id))), big.mark = ",", 
          trim = TRUE)` products, 
`r length(unique(pull(instacart, department_id)))` departments 
and `r length(unique(pull(instacart, aisle_id)))` aisles in the instacart
data set. 

Other key variables include product_name, the product name of the ordered item, 
and order_dow, an encoded variable indicating the day of the week of the order.

Here is some info on the different products that were ordered in the
instacart data set:

*   `r pull(filter(instacart, order_id == 1), product_name)[1]` for order_id 
1 was ordered. This item is located in the 
`r pull(filter(instacart, order_id == 1), aisle)[1]` aisle in
the `r pull(filter(instacart, order_id == 1), department)[1]` department.
*   `r pull(filter(instacart, order_id == 38), product_name)[1]` for order_id 
38 was ordered. This item is located in the 
`r pull(filter(instacart, order_id == 38), aisle)[1]` aisle in
the `r pull(filter(instacart, order_id == 38), department)[1]` department.
*   Queso Fresco was ordered in 
`r nrow(filter(instacart, product_name == "Queso Fresco"))` different orders.


Additionally, we can organize the data set from the aisle with the most items 
ordered to the aisle with the least items ordered. Let's focus on the aisles 
with the most items ordered.

```{r}
# Grouping by aisle and summarizing the total number of items ordered for the
# aisle arranged from most items ordered to least items ordered
(most_items_ordered_aisles <- instacart %>%
                                group_by(aisle_id) %>%
                                summarize(items_ordered = n()) %>%
                                arrange(desc(items_ordered)))
```

From this table, we see that aisle number 
`r pull(most_items_ordered_aisles, aisle_id)[1]` is the aisle with the most
ordered items, followed by aisles 
`r pull(most_items_ordered_aisles, aisle_id)[2]` 
and `r pull(most_items_ordered_aisles, aisle_id)[3]`.

Next, we can create a plot showing the number of items ordered in each aisle.
We will limit this to aisles with more than 10,000 items ordered.

```{r}
# Bar graph showing the total number of items ordered in each aisle.
# Bars ordered largest to smallest from left to right.
# X-tick labels are rotated 90 degrees to provide space for all of the aisle #s
  most_items_ordered_aisles %>%
    filter(items_ordered > 10000) %>%
    ggplot(aes(x = reorder(aisle_id, -items_ordered), y = items_ordered)) +
      geom_bar(stat = "identity", width = 0.5) +
      labs(
      title = "Number of Items Ordered in each Aisle
               for Aisles with Items Ordered over 10,000",
      x = "Aisle Number",
      y = "Total Items Ordered"
    ) +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
          plot.title = element_text(hjust = 0.5)) +
    scale_y_continuous(labels = scales::comma)
```

Next, we can make a table of the three most popular items in each aisle.

```{r message = FALSE}
# Get order frequencies for each unique group (aisle number, product name)
# Filter for the three largest order frequencies for each aisle
instacart %>%
  group_by(aisle, product_name) %>%
  summarize(order_freq = n()) %>%
  slice_max(order_by = order_freq, n = 3) %>%
  knitr::kable()
```
Next, we will make a table showing the mean hour of the day at which Pink Lady 
Apples and Coffee Ice Cream are ordered on each day of the week. For this
table, we are going to recode the day of the week variable (order_dow). It has 
an encoding of 0-6 and we are going to assume that this encoding maps to the 
day of the week Sunday to Saturday respectively. This assumption is based on 
guidance from a TA response to my question about this encoding on the P8105 
class discussion board for homework #3. The instacart data dictionary 
unfortunately does not specify the encoding for the order_dow variable.

```{r message = FALSE}
# Filter for products of interest
# Recode order_dow (0-6) to (Sunday-Saturday) and convert to factor
# Summarize the mean hour of the day at which the products are ordered for
# each day. The group of interest here is the product name and the day of
# the week.
# Pivot the day so that each column is a day of the week, each row is a product
# name and each cell is the summary statistic
instacart %>%
  filter(product_name == "Pink Lady Apples" |
         product_name == "Coffee Ice Cream") %>%
  mutate(order_dow = recode(order_dow, 
                            `0` = "Sunday",
                            `1` = "Monday",
                            `2` = "Tuesday",
                            `3` = "Wednesday",
                            `4` = "Thursday",
                            `5` = "Friday",
                            `6` = "Saturday")) %>%
  mutate(order_dow = factor(order_dow,
                            levels = c("Sunday",
                                       "Monday",
                                       "Tuesday",
                                       "Wednesday",
                                       "Thursday",
                                       "Friday",
                                       "Saturday"))) %>%
  group_by(order_dow, product_name) %>%
  summarize(avg_time = round(mean(order_hour_of_day), 2)) %>%
  pivot_wider(names_from = "order_dow",
              values_from = "avg_time") %>%
  knitr::kable()
```

#### **Problem 2**

First, let's load the BRFSS data.

```{r}
data("brfss_smart2010")
```

Next, let's clean the data.
```{r}
brfss_cleaned <- brfss_smart2010 %>%
  janitor::clean_names()
```

Let's create a tibble focused on the overall health topic in the BRFSS data set.
```{r}
# Filter topic for overall health, then mutate the response to be an ordered
# factor
brfss_overall_health <- brfss_cleaned %>%
                          filter(topic == "Overall Health") %>%
                          mutate(response = factor(response,
                                                   levels = c("Poor",
                                                              "Fair",
                                                              "Good",
                                                              "Very good",
                                                              "Excellent"),
                                                   ordered = TRUE))
```

Using this table focused on overall health, we can see which states were
observed in seven or more locations in 2002.

```{r}
# Filter to 2002 data
# Group by state
# Get all unique locations within the state
# Filter for unique locations greater or equal to 7 per state
brfss_overall_health %>%
  filter(year == 2002) %>%
  group_by(locationabbr) %>%
  summarize(
    num_locations = n_distinct(locationdesc)
  ) %>%
  filter(num_locations >= 7) %>%
  knitr::kable()
```

We can also use this overall health table to see which states were
observed in seven or more locations in 2010.

```{r}
# Filter to 2010 data
# Group by state
# Get all unique locations with the state
# Filter for unique locations greater or equal to 7 per state
brfss_overall_health %>%
  filter(year == 2010) %>%
  group_by(locationabbr) %>%
  summarize(
    num_locations = n_distinct(locationdesc)
  ) %>%
  filter(num_locations >= 7) %>%
  knitr::kable()
```

There are more states with 7 or more locations in 2010 than there are in 2002.
FL, MA, NC, NJ and PA are in both tables.

Next, let's use the table focused on overall health to arrange a new table
of only Excellent responses and only contains variables year, state, and 
an aggregate variable that is the mean data_value among locations within the 
state. 

```{r message = FALSE}
# Filter for Excellent Responses
# Group by State and Year
# Get a mean data value for each state-year group
brfss_excellent_responses <- brfss_overall_health %>%
                                filter(response == "Excellent") %>%
                                group_by(locationabbr, year) %>%
                                summarize(
                                  avg_data_value = round(mean(data_value, 
                                                         na.rm = TRUE),1)
                                )
knitr::kable(brfss_excellent_responses)
```

Let's plot this new table of excellent health responses. We will plot
the year on the x-axis and mean data_value on the y-axis. We will plot one
line for each state visualizing how each state's mean data_value changes
over time. Further, we will color each state's line a different color
and label the state's color in the legend located on the right side of the
plot.

```{r}
# Plot excellent responses with year on the x-axis and average data value on the 
# y-axis. Plot one line for each state and color each state using the 
# viridis color package.
brfss_excellent_responses %>%
  ggplot(aes(x = year, y = avg_data_value, color = locationabbr)) +
    geom_line() +
    viridis::scale_color_viridis(discrete = TRUE, name = "State") +
  labs(
    title = "Average Data Value by Year stratified by State",
    y = "Average Data Value",
    x = "Year",
  ) +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))
```

We see in this plot that most average data values start between 20 and 27.5 in 
2002 and end between 17.5 and 25 in 2010. As such, there appears to be a 
slight decline in the average data value across states over time if we focus on 
the central tendency of the average data values for all states.

Next, we will create a two-panel plot, for years 2006 and 2010, visualizing the
distribution of data_value for responses, from poor to excellent, among 
locations in NY State.

Let's use our overall health table to complete this task. We will create
boxplots to visualize the distribution of data_value for each response for 
years 2006 and 2010.

```{r}
# Filter for NY state and years 2006 and 2010
# Create a boxplot with each health response on the x-axis and the
# data_value on the y-axis
# Use facet_wrap() to create 2 panels: one for 2006 and one for 2010
brfss_overall_health %>%
  filter(locationabbr == "NY", year == 2006 | year == 2010) %>%
  ggplot(aes(x = response, y = data_value)) +
    geom_boxplot() +
    facet_wrap(~year) +
    theme_bw() +
  labs(
    title = "NY Distribution of data_value by Response for 2006 and 2010",
    x = "Response"
  ) +
  theme(plot.title = element_text(hjust = 0.5))
```
There are some disparities between the distribution of data_value at locations
in NY between 2006 and 2010. One difference is that the data_value values of 
"Good" and "Very good" response groups are more similar in 2006 than they are in 
2010. However, overall, the distribution of data_value in 2006 and 2010 for 
locations in NY is quite similar. Poor responses have the lowest data_value 
values, followed by Fair responses, Excellent responses, Good responses and 
lastly Very good responses with the highest data_value values. This ordering
of the data_value values across response groups is the same for both 2006 and 
2010 across locations in NY. It should be noted that although we are comparing 
the distribution of data_value across response groups in years 2006 and 2010, 
no statistical tests were performed and the groups were strictly compared 
visually using the boxplots above.

#### **Problem 3**

Let's begin by loading the data.
```{r message = FALSE}
accel_df <- read_csv(here("hw3_data", "accel_data.csv"))
```

The activity variable is displayed across multiple columns from
activity.1 to activity.1440. We can pivot the table to have this activity
variable in one column.

```{r}
# Pivot from wide to long, putting all activity count indices in one column
# and all the activity count measurements in a separate column.
accel_df <- accel_df %>%
  pivot_longer(
    activity.1:activity.1440,
    names_to = "activity_count_index",
    values_to = "activity_count_reading") %>%
  mutate(activity_count_index = as.integer(str_replace(activity_count_index,
                                            "activity.",
                                            "")))
```

We can next add a logical variable that is TRUE if the day is on a weekend day
and FALSE if the day is on a weekday.

```{r}
accel_df <- accel_df %>%
  mutate(is_weekend = ifelse(day == "Saturday" | day == "Sunday",
                             TRUE, FALSE))
```

Additionally, we can convert the day ID variable to be of type integer and
the day variable to be a factor.

```{r}
# Adjusting the types of two variables
accel_df <- accel_df %>%
  mutate(day_id = as.integer(day_id),
         day = factor(day, levels = c("Monday", 
                                      "Tuesday",
                                      "Wednesday",
                                      "Thursday",
                                      "Friday",
                                      "Saturday",
                                      "Sunday")))
```

The resulting data set has `r format(nrow(accel_df), big.mark = ",")` rows and 
`r ncol(accel_df)` columns, with the columns being week, day_id, day, 
activity_count_index, activity_count_reading, is_weekend. There is one row
for every activity count reading, and there is one activity count
reading for every minute of everyday for five weeks.

Now, let's use our tidied data set to aggregate across minutes to create a
total activity variable for each day. We will show the results in a table.
```{r}
# Group by each day and get a sum for the activity count reading for the day
accel_df %>%
  group_by(day_id) %>%
  summarize(total_activity_day = sum(activity_count_reading)) %>%
  knitr::kable()
```

It appears activity started off low for the first three days. After the first 
three days, activity increased. It's difficult to discern any trends after the 
first three days.

Next, let's make a single-panel plot that visualizes the twenty-four hour 
activity time courses for each day. We will color by day of the week.

```{r}
# Set group in ggplot to plot one line for each day
accel_df %>%
  ggplot(aes(x = activity_count_index, y = activity_count_reading,
             group = day_id, color = day)) +
  geom_line() +
  labs(
    x = "Minute of the Day",
    y = "Activity Count Reading",
    title = "Activity Count Reading as a function of Minute of the Day
             stratified by Day of the Week",
    color = "Day"
  ) +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))
```
From the graph, it appears that the activity count reading increases from
early in the day to later in the day. We observe some spikes in activity
counts that are typically on Fridays, Saturdays or Sundays.

