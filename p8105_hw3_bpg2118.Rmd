---
title: "p8105_hw3_bpg2118"
author: "Benjamin Goebel"
date: "10/20/2021"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(here)
```

#### **Problem 1**

First, let's read in the instacart data.
```{r}
data("instacart")
```

This data set is `r format(nrow(instacart), big.mark = ",", trim = TRUE)` rows 
and `r ncol(instacart)` columns. 
Each observation in this data set represents a product in each order.
Some key variables include order_id, product_id, department_id and aisle_id; 
these are the respective IDs for the order, product, department and aisle.
There are 
`r format(length(unique(pull(instacart, order_id))), big.mark = ",", 
          trim = TRUE)` orders, 
`r format(length(unique(pull(instacart, product_id))), big.mark = ",", 
          trim = TRUE)` products, 
`r length(unique(pull(instacart, department_id)))` departments 
and `r length(unique(pull(instacart, aisle_id)))` aisles. 

Here is some info on the different products that were ordered in the
instacart data set:

*   `r pull(filter(instacart, order_id == 1), product_name)[1]` for order_id 
1 was ordered. This item is located in the 
`r pull(filter(instacart, order_id == 1), aisle)[1]` aisle in
the `r pull(filter(instacart, order_id == 1), department)[1]` department.
*   `r pull(filter(instacart, order_id == 38), product_name)[1]` for order_id 
38 was ordered. This item is located in the 
`r pull(filter(instacart, order_id == 38), aisle)[1]` aisle in
the `r pull(filter(instacart, order_id == 38), department)[1]` department.
*   Queso Fresco was ordered in 
`r nrow(filter(instacart, product_name == "Queso Fresco"))` different orders.


Additionally, we can organize the data set from the aisle with the most items 
ordered to the aisle with the least items ordered. Let's focus on the aisles 
with the most items ordered.

```{r}
# Grouping by aisle and summarizing the total number of items ordered for the
# aisle arranged from largest to smallest
(most_items_ordered_aisles <- instacart %>%
                                group_by(aisle_id) %>%
                                summarize(items_ordered = n()) %>%
                                arrange(desc(items_ordered)))
```

From this table, we see that aisle number 
`r pull(most_items_ordered_aisles, aisle_id)[1]` is the aisle with the most
ordered items, followed by aisles 
`r pull(most_items_ordered_aisles, aisle_id)[2]` 
and `r pull(most_items_ordered_aisles, aisle_id)[3]`.

Next, we can create a plot showing the number of items ordered in each aisle.
We will limit this to aisles with more than 10,000 items ordered.

```{r}
# Bar graph with bars ordered largest to smallest from left to right
  most_items_ordered_aisles %>%
    filter(items_ordered > 10000) %>%
    ggplot(aes(x = reorder(aisle_id, -items_ordered), y = items_ordered)) +
      geom_bar(stat = "identity", width = 0.5) +
      labs(
      title = "Number of Items Ordered in each Aisle
               for Aisles with Orders over 10,000",
      x = "Aisle Number",
      y = "Items Ordered"
    ) +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
          plot.title = element_text(hjust = 0.5)) +
    scale_y_continuous(labels = scales::comma)
```

Next, we can make a table of the three most popular items in each aisle.

```{r message = FALSE}
instacart %>%
  group_by(aisle, product_name) %>%
  summarize(order_freq = n()) %>%
  slice_max(order_by = order_freq, n = 3) %>%
  knitr::kable()
```
Next, we will make a table showing the mean hour of the day at which Pink Lady 
Apples and Coffee Ice Cream are ordered on each day of the week. For this
table, we are going to recode the day of the week variable (order_dow). It has 
an encoding of 0-6 and we are going to assume that this encoding maps to the 
day of the week Sunday to Saturday. This assumption is based on guidance from
a TA response to my question about this on the P8105 class discussion board
for homework #3. The instacart data dictionary unfortunately does not specify 
the encoding for the order_dow variable.

```{r message = FALSE}
# Filter for products of interest
# Recode order_dow (0-6) to (Sunday-Saturday) and convert to factor
# Summarize the mean hour of the day at which the products are ordered for
# each day. The group of interest here is the product name and the day of
# the week.
# Pivot the day so that each column is a day of the week, each row is a product
# name and each cell is the summary statistic
instacart %>%
  filter(product_name == "Pink Lady Apples" |
         product_name == "Coffee Ice Cream") %>%
  mutate(order_dow = recode(order_dow, 
                            `0` = "Sunday",
                            `1` = "Monday",
                            `2` = "Tuesday",
                            `3` = "Wednesday",
                            `4` = "Thursday",
                            `5` = "Friday",
                            `6` = "Saturday")) %>%
  mutate(order_dow = factor(order_dow,
                            levels = c("Sunday",
                                       "Monday",
                                       "Tuesday",
                                       "Wednesday",
                                       "Thursday",
                                       "Friday",
                                       "Saturday"))) %>%
  group_by(order_dow, product_name) %>%
  summarize(avg_time = round(mean(order_hour_of_day), 2)) %>%
  pivot_wider(names_from = "order_dow",
              values_from = "avg_time") %>%
  knitr::kable()
```

#### **Problem 2**

First, let's load the BRFSS data.

```{r}
data("brfss_smart2010")
```

Next, let's clean the data.
```{r}
brfss_cleaned <- brfss_smart2010 %>%
  janitor::clean_names()
```

Let's create a tibble solely focused on the overall health topic
in the BRFSS data set.
```{r}
# Filter topic on overall health, then mutate the response to be an ordered
# factor
brfss_overall_health <- brfss_cleaned %>%
                          filter(topic == "Overall Health") %>%
                          mutate(response = factor(response,
                                                   levels = c("Poor",
                                                              "Fair",
                                                              "Good",
                                                              "Very good",
                                                              "Excellent"),
                                                   ordered = TRUE))
                                 
                                                   
```

Using this table focused on overall health, we can see which states were
observed in seven or more locations in 2002.

```{r}
# Filter to 2002 data
# Group by state
# Get all unique locations with the state
# Filter for unique locations greater or equal to 7 per state
brfss_overall_health %>%
  filter(year == 2002) %>%
  group_by(locationabbr) %>%
  summarize(
    num_locations = n_distinct(locationdesc)
  ) %>%
  filter(num_locations >= 7) %>%
  knitr::kable()
```

We can also use this table to see which states were
observed in seven or more locations in 2010.

```{r}
# Filter to 2010 data
# Group by state
# Get all unique locations with the state
# Filter for unique locations greater or equal to 7 per state
brfss_overall_health %>%
  filter(year == 2010) %>%
  group_by(locationabbr) %>%
  summarize(
    num_locations = n_distinct(locationdesc)
  ) %>%
  filter(num_locations >= 7) %>%
  knitr::kable()
```

Next, let's use the table focused on overall health to arrange a new table
of only Excellent responses and only contains variables year, state, and 
an aggregated variable that is the mean data_value among locations within a 
state. 

```{r message = FALSE}
# Filter on Excellent Responses
# Group by State and Year
# Get a mean data value for each state-year group
brfss_excellent_responses <- brfss_overall_health %>%
                                filter(response == "Excellent") %>%
                                group_by(locationabbr, year) %>%
                                summarize(
                                  avg_data_value = round(mean(data_value, 
                                                         na.rm = TRUE),1)
                                )
knitr::kable(brfss_excellent_responses)
```

Let's plot this new table of excellent health responses. We will plot
the year on the x-axis, mean data_value on the y-axis. We will plot one
line for each state visualizing how each state's mean data_value changes
through the years. Further, we will color each state's a different color
and label the state's color in the legend located on the right side of the
plot.

```{r}
# Plot excellent responses with year on the x-axis, average data value on the 
# y-axis. Plot one line for each state and color each state using the 
# viridis color package.
brfss_excellent_responses %>%
  ggplot(aes(x = year, y = avg_data_value, color = locationabbr)) +
    geom_line() +
    viridis::scale_color_viridis(discrete = TRUE, name = "State") +
  labs(
    title = "Average Data Value across Years stratified by State",
    y = "Average Data Value",
    x = "Years",
  ) +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))
```

Next, we will create a two-panel plot, for years 2006 and 2010, visualizing the
distribution of data_value for responses (“Poor” to “Excellent”) among locations 
in NY State.

Let's use our overall health table to complete this task. We will create a
boxplot to visualize the distribution of data_value for each response.

```{r}
# Filter for NY state and years 2002 and 2006
# Create a boxplot with each health response on the x-axis and the
# distribution of the data_value on the y-axis
brfss_overall_health %>%
  filter(locationabbr == "NY", year == 2002 | year == 2006) %>%
  ggplot(aes(x = response, y = data_value)) +
    geom_boxplot() +
    facet_wrap(~year) +
    theme_bw() +
  labs(
    title = "NY Distribution of data_value by Response for 2002 and 2006",
    x = "Response"
  ) +
  theme(plot.title = element_text(hjust = 0.5))
```

#### **Problem 3**

Let's begin by loading the data.
```{r message = FALSE}
accel_df <- read_csv(here("hw3_data", "accel_data.csv"))
```

The activity variable is displayed across multiple columns from
activity.1 to activity.1440. We can pivot the table to have this activity
variable in one column.

```{r}
# Pivot from wide to long, putting all activity count indices in one column
# and all the activity count measurements in a separate column.
accel_df <- accel_df %>%
  pivot_longer(
    activity.1:activity.1440,
    names_to = "activity_count_index",
    values_to = "activity_count_reading") %>%
  mutate(activity_count_index = as.integer(str_replace(activity_count_index,
                                            "activity.",
                                            "")))
```

We can next add a logical variable that is TRUE if the day is on a weekend day
and FALSE if the day is on a weekday.

```{r}
accel_df <- accel_df %>%
  mutate(is_weekend = ifelse(day == "Saturday" | day == "Sunday",
                             TRUE, FALSE))
```

Additionally, we can convert the day ID variable to be of type integer and
the day variable to be a factor.

```{r}
# Adjusting the types of two variables
accel_df <- accel_df %>%
  mutate(day_id = as.integer(day_id),
         day = factor(day, levels = c("Monday", 
                                      "Tuesday",
                                      "Wednesday",
                                      "Thursday",
                                      "Friday",
                                      "Saturday",
                                      "Sunday")))
```

The resulting data set has `r format(nrow(accel_df), big.mark = ",")` rows and 
`r ncol(accel_df)` columns, with the columns being week, day_id, day, 
activity_count_index, activity_count_reading, is_weekend. There is one row
for every activity count reading, and there is one activity count
reading for every minute of everyday for five weeks.

Now, let's use our tidied data set to aggregate across minutes to create a
total activity variable for each day. We will show the results in a table.
```{r}
# Group by each day and get a sum for the activity count reading for the day
accel_df %>%
  group_by(day_id) %>%
  summarize(total_activity_day = sum(activity_count_reading)) %>%
  knitr::kable()
```

It appears activity started off low for the first three days. After the first 
three days, activity increased. It's difficult to discern any trends after the 
first three days.

Next, let's make a single-panel plot that visualizes the twenty-four hour 
activity time courses for each day. We will color by day of the week.

```{r}
# Set group in ggplot to plot one line for each day
accel_df %>%
  ggplot(aes(x = activity_count_index, y = activity_count_reading,
             group = day_id, color = day)) +
  geom_line() +
  labs(
    x = "Minute of the Day",
    y = "Activity Count Reading",
    title = "Activity Count Reading as a function of Minute of the Day
             stratified by Day of the Week",
    color = "Day"
  ) +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))
```
From the graph, it appears that the activity count reading increases from
early in the day to later in the day. We observe some spikes in activity
counts that are typically on Fridays, Saturdays or Sundays.

